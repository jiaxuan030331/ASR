import os
import time
import threading
import json
import functools
import logging
from enum import Enum
from typing import List, Optional
import torch
import numpy as np
from websockets.sync.server import serve
from websockets.exceptions import ConnectionClosed
from whisper_live.vad import VoiceActivityDetector
from whisper_live.transcriber import WhisperModel
from whisper_live import utils
from collections import namedtuple
import requests

try:
    from whisper_live.transcriber_tensorrt import WhisperTRTLLM
except Exception:
    pass

# logging.basicConfig(level=logging.INFO)
utils.init_log('./logs/whiser.log', level=logging.INFO, stdout=True, backup=30)


class ModelManager:
    SINGLE_MODEL_LOCK = threading.Lock()

    def __init__(self, model_size_or_path, model_size=[{'idx': 0, 'size': 2}]):
        self.models = []
        self.remove_models = []
        device = "cuda" if torch.cuda.is_available() else "cpu"
        for model_item in model_size:
            idx = model_item['idx']
            nums = model_item['size']
            for i in range(nums):
                transcriber = self.create_model(device, model_size_or_path, idx)
                model_info = {'key': idx * 100 + (i + 1), 'value': transcriber}
                self.models.append(model_info)

    def create_model(self, device, model_size_or_path, device_index=0):
        transcriber = WhisperModel(
            model_size_or_path,
            device=device,
            compute_type="int8" if device == "cpu" else "float16",
            # cpu_threads=16,
            # num_workers=4,
            local_files_only=False,
        )
        return transcriber

    def get_model(self):
        if self.models:
            ModelManager.SINGLE_MODEL_LOCK.acquire()
            models = self.models.pop(0)
            logging.info("ModelManager GET: " + str(models['key']))
            self.remove_models.append(models)
            ModelManager.SINGLE_MODEL_LOCK.release()
            return models
        else:
            return None

    def add_model(self, model_id):
        ModelManager.SINGLE_MODEL_LOCK.acquire()
        mod_idx = -1
        idx = 0
        for model_info in self.remove_models:
            key = model_info.get('key')
            if key == model_id:
                mod_idx = idx
                break
            idx += 1
        models = self.remove_models.pop(mod_idx)
        logging.info("ModelManager REMOVE: " + str(models['key']))
        self.models.append(models)
        ModelManager.SINGLE_MODEL_LOCK.release()


class ClientManager:
    def __init__(self, max_clients=40, max_connection_time=800):
        self.clients = {}
        self.start_times = {}
        self.max_clients = max_clients
        self.max_connection_time = max_connection_time

    def add_client(self, websocket, client):
        self.clients[websocket] = client
        self.start_times[websocket] = time.time()

    def get_client(self, websocket):
        if websocket in self.clients:
            return self.clients[websocket]
        return False

    def remove_client(self, websocket):
        client = self.clients.pop(websocket, None)
        if client:
            client.cleanup()
        self.start_times.pop(websocket, None)

    def get_wait_time(self):
        wait_time = None
        for start_time in self.start_times.values():
            current_client_time_remaining = self.max_connection_time - (time.time() - start_time)
            if wait_time is None or current_client_time_remaining < wait_time:
                wait_time = current_client_time_remaining
        return wait_time / 60 if wait_time is not None else 0

    def is_server_full(self, websocket, options):
        if len(self.clients) >= self.max_clients:
            wait_time = self.get_wait_time()
            response = {"uid": options["uid"], "status": "WAIT", "message": wait_time}
            websocket.send(json.dumps(response))
            return True
        return False

    def is_client_timeout(self, websocket):
        elapsed_time = time.time() - self.start_times[websocket]
        print('is_client_timeout :' + str(elapsed_time))
        if elapsed_time >= self.max_connection_time:
            self.clients[websocket].disconnect()
            logging.warning(f"Client with uid '{self.clients[websocket].client_uid}' disconnected due to overtime.")
            return True
        return False


class BackendType(Enum):
    FASTER_WHISPER = "faster_whisper"
    TENSORRT = "tensorrt"

    @staticmethod
    def valid_types() -> List[str]:
        return [backend_type.value for backend_type in BackendType]

    @staticmethod
    def is_valid(backend: str) -> bool:
        return backend in BackendType.valid_types()

    def is_faster_whisper(self) -> bool:
        return self == BackendType.FASTER_WHISPER

    def is_tensorrt(self) -> bool:
        return self == BackendType.TENSORRT


class TranscriptionServer:
    RATE = 16000

    def __init__(self):
        self.client_manager = ClientManager()
        self.model_manager = None
        self.no_voice_activity_chunks = 0
        self.use_vad = True
        self.single_model = False
        self.num_workers = 1

    def initialize_client(
            self, websocket, options, faster_whisper_custom_model_path,
            whisper_tensorrt_path, trt_multilingual
    ):
        client: Optional[ServeClientBase] = None

        if self.backend.is_tensorrt():
            try:
                client = ServeClientTensorRT(
                    websocket,
                    multilingual=trt_multilingual,
                    language=options["language"],
                    task=options["task"],
                    client_uid=options["uid"],
                    model=whisper_tensorrt_path,
                    single_model=self.single_model,
                )
                logging.info("Running TensorRT backend.")
            except Exception as e:
                logging.error(f"TensorRT-LLM not supported: {e}")
                self.client_uid = options["uid"]
                websocket.send(json.dumps({
                    "uid": self.client_uid,
                    "status": "WARNING",
                    "message": "TensorRT-LLM not supported on Server yet. "
                               "Reverting to available backend: 'faster_whisper'"
                }))
                self.backend = BackendType.FASTER_WHISPER

        elif self.backend.is_faster_whisper():
            if faster_whisper_custom_model_path is not None and os.path.exists(faster_whisper_custom_model_path):
                logging.info(f"Using custom model {faster_whisper_custom_model_path}")
                options["model"] = faster_whisper_custom_model_path
            client = ServeClientFasterWhisper(
                websocket,
                language=options["language"],
                task=options["task"],
                client_uid=options["uid"],
                model=options["model"],
                initial_prompt=options.get("initial_prompt"),
                vad_parameters=options.get("vad_parameters"),
                use_vad=self.use_vad,
                single_model=self.single_model,
                model_manager=self.model_manager
            )
            logging.info("Running faster_whisper backend.")

        if client is None:
            raise ValueError(f"Backend type {self.backend.value} not recognised or not handled.")

        self.client_manager.add_client(websocket, client)

    def get_audio_from_websocket(self, websocket):
        frame_data = websocket.recv()
        print('get audio from websocket ------------------')
        if frame_data == b"END_OF_AUDIO":
            print('get audio from websocket ------------------ END')
            return False
        #raw_data = np.frombuffer(buffer=frame_data, dtype=np.int16)
        #raw_data = raw_data.astype(np.float32) / 32768.0

        return np.frombuffer(frame_data, dtype=np.float32)
    def get_audio_bytes_from_websocket(self, websocket):
        frame_data = websocket.recv()
        print('get audio from websocket ------------------')
        if frame_data == b"END_OF_AUDIO":
            print('get audio from websocket ------------------ END')
            return False
        raw_data = np.frombuffer(buffer=frame_data, dtype=np.int16)
        raw_data = raw_data.astype(np.float32) / 32768.0
        return raw_data
        #return np.frombuffer(frame_data, dtype=np.float32)

    def handle_new_connection(self, websocket, faster_whisper_custom_model_path,
                              whisper_tensorrt_path, trt_multilingual):
        try:
            logging.info("New client connected")
            options = websocket.recv()
            options = json.loads(options)
            self.use_vad = options.get('use_vad')
            if self.client_manager.is_server_full(websocket, options):
                websocket.close()
                return False  # Indicates that the connection should not continue

            if self.backend.is_tensorrt():
                self.vad_detector = VoiceActivityDetector(frame_rate=self.RATE)
            self.initialize_client(websocket, options, faster_whisper_custom_model_path,
                                   whisper_tensorrt_path, trt_multilingual)
            return True
        except json.JSONDecodeError:
            logging.error("Failed to decode JSON from client")
            return False
        except ConnectionClosed:
            logging.info("Connection closed by client")
            return False
        except Exception as e:
            logging.error(f"Error during new connection initialization: {str(e)}")
            return False

    def process_audio_frames(self, websocket):
        frame_np = self.get_audio_bytes_from_websocket(websocket)
        client = self.client_manager.get_client(websocket)
        if frame_np is False:
            if self.backend.is_tensorrt():
                client.set_eos(True)
            client.update_frames_size()
            whileCount = 200
            while whileCount > 0:
                if client.canExit is True:
                    print('get audio from websocket ------END_OF_AUDIO')
                    break
                time.sleep(0.05)
                whileCount -= 1
            return False

        if self.backend.is_tensorrt():
            voice_active = self.voice_activity(websocket, frame_np)
            if voice_active:
                self.no_voice_activity_chunks = 0
                client.set_eos(False)
            if self.use_vad and not voice_active:
                self.no_voice_activity_chunks += 1
                if self.no_voice_activity_chunks > 10:
                    self.notify_no_voice_activity(websocket)
                    return False  # End the connection

        client.add_frames(frame_np)
        return True

    def notify_no_voice_activity(self, websocket):
        client = self.client_manager.get_client(websocket)
        if client:
            client.websocket.send(json.dumps({
                "uid": client.client_uid,
                "message": "NO_VOICE_ACTIVITY"
            }))

    def recv_audio(self,
                   websocket,
                   backend: BackendType = BackendType.FASTER_WHISPER,
                   faster_whisper_custom_model_path=None,
                   whisper_tensorrt_path=None,
                   trt_multilingual=False):
        self.backend = backend
        t1 = time.time()
        if not self.handle_new_connection(websocket, faster_whisper_custom_model_path,
                                          whisper_tensorrt_path, trt_multilingual):
            return

        print('connecting ----------- ok :' + str(time.time() - t1))
        try:
            while not self.client_manager.is_client_timeout(websocket):
                if not self.process_audio_frames(websocket):
                    break
        except ConnectionClosed:
            logging.info("Connection closed by client")
        except Exception as e:
            logging.error(f"Unexpected error: {str(e)}")
        finally:
            if self.client_manager.get_client(websocket):
                time.sleep(0.1)
                self.cleanup(websocket)
                websocket.close()
            del websocket

    def run(self,
            host,
            port=9090,
            backend="tensorrt",
            faster_whisper_custom_model_path=None,
            whisper_tensorrt_path=None,
            trt_multilingual=False,
            single_model=False):
        if faster_whisper_custom_model_path is not None and not os.path.exists(faster_whisper_custom_model_path):
            raise ValueError(f"Custom faster_whisper model '{faster_whisper_custom_model_path}' is not a valid path.")
        if whisper_tensorrt_path is not None and not os.path.exists(whisper_tensorrt_path):
            raise ValueError(f"TensorRT model '{whisper_tensorrt_path}' is not a valid path.")
        if single_model:
            if faster_whisper_custom_model_path or whisper_tensorrt_path:
                logging.info("Custom model option was provided. Switching to single model mode.")
                self.single_model = True
            else:
                logging.info("Single model mode currently only works with custom models.")
        if not BackendType.is_valid(backend):
            raise ValueError(f"{backend} is not a valid backend type. Choose backend from {BackendType.valid_types()}")

        # self.model_manager = ModelManager(faster_whisper_custom_model_path)
        with serve(
                functools.partial(
                    self.recv_audio,
                    backend=BackendType(backend),
                    faster_whisper_custom_model_path=faster_whisper_custom_model_path,
                    whisper_tensorrt_path=whisper_tensorrt_path,
                    trt_multilingual=trt_multilingual
                ),
                host,
                port
        ) as server:
            server.serve_forever()

    def voice_activity(self, websocket, frame_np):
        if not self.vad_detector(frame_np):
            return False
        return True

    def cleanup(self, websocket):
        if self.client_manager.get_client(websocket):
            self.client_manager.remove_client(websocket)


class ServeClientBase(object):
    RATE = 16000
    SERVER_READY = "SERVER_READY"
    DISCONNECT = "DISCONNECT"

    def __init__(self, client_uid, websocket):
        self.client_uid = client_uid
        self.websocket = websocket
        self.frames = b""
        self.timestamp_offset = 0.0
        self.frames_np = None
        self.frames_np_size = 0
        self.frames_offset = 0.0
        self.text = []
        self.current_out = ''
        self.prev_out = ''
        self.t_start = None
        self.exit = False
        self.canExit = False
        self.same_output_threshold = 0
        self.show_prev_out_thresh = 5
        self.add_pause_thresh = 3
        self.transcript = []
        self.send_last_n_segments = 10
        self.lock = threading.Lock()
        self.http_url = 'http://127.0.0.1:8001'

    def update_frames_size(self):
        if self.frames_np is not None:
            self.frames_np_size = len(self.frames_np)

    def init_frames_size(self):
        self.frames_np_size = 0

    def speech_to_text(self):
        raise NotImplementedError

    def transcribe_audio(self):
        raise NotImplementedError

    def handle_transcription_output(self):
        raise NotImplementedError

    def add_frames(self, frame_np):
        self.canExit = False
        self.init_frames_size()
        self.lock.acquire()
        if self.frames_np is not None and self.frames_np.shape[0] > 45 * self.RATE:
            self.frames_offset += 30.0
            self.frames_np = self.frames_np[int(30 * self.RATE):]
            if self.timestamp_offset < self.frames_offset:
                self.timestamp_offset = self.frames_offset
        if self.frames_np is None:
            self.frames_np = frame_np.copy()
        else:
            self.frames_np = np.concatenate((self.frames_np, frame_np), axis=0)

        print('add_frames ----- length ' + str(len(self.frames_np)))
        self.lock.release()

    def clip_audio_if_no_valid_segment(self):
        if self.frames_np[int((self.timestamp_offset - self.frames_offset) * self.RATE):].shape[0] > 25 * self.RATE:
            duration = self.frames_np.shape[0] / self.RATE
            self.timestamp_offset = self.frames_offset + duration - 5

    def get_audio_chunk_for_processing(self):
        samples_take = max(0, (self.timestamp_offset - self.frames_offset) * self.RATE)
        input_bytes = self.frames_np[int(samples_take):].copy()
        duration = input_bytes.shape[0] / self.RATE
        return input_bytes, duration, len(self.frames_np)

    def prepare_segments(self, last_segment=None):
        segments = []
        if len(self.transcript) >= self.send_last_n_segments:
            segments = self.transcript[-self.send_last_n_segments:].copy()
        else:
            segments = self.transcript.copy()
        if last_segment is not None:
            segments = segments + [last_segment]
        return segments

    def get_audio_chunk_duration(self, input_bytes):
        return input_bytes.shape[0] / self.RATE

    def send_transcription_to_client(self, segments, is_end=False):
        try:
            self.websocket.send(
                json.dumps({
                    "uid": self.client_uid,
                    "segments": segments,
                    "is_end": is_end
                })
            )
        except Exception as e:
            logging.error(f"[ERROR]: Sending data to client: {e}")

    def disconnect(self):
        self.websocket.send(json.dumps({
            "uid": self.client_uid,
            "message": self.DISCONNECT
        }))

    def cleanup(self):
        logging.info("Cleaning up.")
        self.exit = True


class ServeClientTensorRT(ServeClientBase):
    SINGLE_MODEL = None
    SINGLE_MODEL_LOCK = threading.Lock()

    def __init__(self, websocket, task="transcribe", multilingual=False, language=None, client_uid=None, model=None,
                 single_model=False):
        super().__init__(client_uid, websocket)
        self.language = language if multilingual else "en"
        self.task = task
        self.eos = False

        if single_model:
            if ServeClientTensorRT.SINGLE_MODEL is None:
                self.create_model(model, multilingual)
                ServeClientTensorRT.SINGLE_MODEL = self.transcriber
            else:
                self.transcriber = ServeClientTensorRT.SINGLE_MODEL
        else:
            self.create_model(model, multilingual)

        self.trans_thread = threading.Thread(target=self.speech_to_text)
        self.trans_thread.start()

        self.websocket.send(json.dumps({
            "uid": self.client_uid,
            "message": self.SERVER_READY,
            "backend": "tensorrt"
        }))

    def create_model(self, model, multilingual, warmup=True):
        self.transcriber = WhisperTRTLLM(
            model,
            assets_dir="assets",
            device="cuda",
            is_multilingual=multilingual,
            language=self.language,
            task=self.task
        )
        if warmup:
            self.warmup()

    def warmup(self, warmup_steps=10):
        logging.info("[INFO:] Warming up TensorRT engine..")
        mel, _ = self.transcriber.log_mel_spectrogram("assets/jfk.flac")
        for i in range(warmup_steps):
            self.transcriber.transcribe(mel)

    def set_eos(self, eos):
        self.lock.acquire()
        self.eos = eos
        self.lock.release()

    def handle_transcription_output(self, last_segment, duration):
        segments = self.prepare_segments({"text": last_segment})
        self.send_transcription_to_client(segments)
        if self.eos:
            self.update_timestamp_offset(last_segment, duration)

    def transcribe_audio(self, input_bytes):
        if ServeClientTensorRT.SINGLE_MODEL:
            ServeClientTensorRT.SINGLE_MODEL_LOCK.acquire()
        logging.info(f"[WhisperTensorRT:] Processing audio with duration: {input_bytes.shape[0] / self.RATE}")
        mel, duration = self.transcriber.log_mel_spectrogram(input_bytes)
        last_segment = self.transcriber.transcribe(
            mel,
            text_prefix=f"<|{self.language}|><|{self.task}|>"
        )
        if ServeClientTensorRT.SINGLE_MODEL:
            ServeClientTensorRT.SINGLE_MODEL_LOCK.release()
        if last_segment:
            self.handle_transcription_output(last_segment, duration)

    def update_timestamp_offset(self, last_segment, duration):
        if not len(self.transcript):
            self.transcript.append({"text": last_segment + " "})
        elif self.transcript[-1]["text"].strip() != last_segment:
            self.transcript.append({"text": last_segment + " "})
        self.timestamp_offset += duration

    def speech_to_text(self):
        while True:
            if self.exit:
                logging.info("Exiting speech to text thread")
                break

            if self.frames_np is None:
                time.sleep(0.02)
                continue

            self.clip_audio_if_no_valid_segment()

            input_bytes, duration = self.get_audio_chunk_for_processing()
            if duration < 0.4:
                continue

            try:
                input_sample = input_bytes.copy()
                logging.info(f"[WhisperTensorRT:] Processing audio with duration: {duration}")
                self.transcribe_audio(input_sample)

            except Exception as e:
                logging.error(f"[ERROR]: {e}")


class ServeClientFasterWhisper(ServeClientBase):
    SINGLE_MODEL = None
    SINGLE_MODEL_LOCK = threading.Lock()

    def __init__(self, websocket, task="transcribe", device=None, language=None, client_uid=None, model="small.en",
                 initial_prompt=None, vad_parameters=None, use_vad=True, single_model=False, model_manager=None):
        super().__init__(client_uid, websocket)
        self.model_sizes = [
            "tiny", "tiny.en", "base", "base.en", "small", "small.en",
            "medium", "medium.en", "large-v2", "large-v3",
        ]
        self.modelsManager = model_manager
        if not os.path.exists(model):
            self.model_size_or_path = self.check_valid_model(model)
        else:
            self.model_size_or_path = model
        self.language = "en" if self.model_size_or_path.endswith("en") else language
        self.task = task
        self.initial_prompt = initial_prompt
        self.vad_parameters = vad_parameters or {"threshold": 0.5}
        self.no_speech_thresh = 0.85

        device = "cuda" if torch.cuda.is_available() else "cpu"

        if self.model_size_or_path is None:
            return

        # if single_model:
        #    if ServeClientFasterWhisper.SINGLE_MODEL is None:
        #        self.create_model(device)
        #        ServeClientFasterWhisper.SINGLE_MODEL = self.transcriber
        #    else:
        #        self.transcriber = ServeClientFasterWhisper.SINGLE_MODEL
        # else:
        #    self.create_model(device)

        self.use_vad = use_vad

        self.trans_thread = threading.Thread(target=self.speech_to_text)
        self.trans_thread.start()
        self.websocket.send(
            json.dumps(
                {
                    "uid": self.client_uid,
                    "message": self.SERVER_READY,
                    "backend": "faster_whisper"
                }
            )
        )

    def create_model(self, device):
        self.transcriber = WhisperModel(
            self.model_size_or_path,
            device=device,
            compute_type="int8" if device == "cpu" else "float16",
            num_workers=self.num_workers,
            local_files_only=False,
        )

    def check_valid_model(self, model_size):
        if model_size not in self.model_sizes:
            self.websocket.send(
                json.dumps(
                    {
                        "uid": self.client_uid,
                        "status": "ERROR",
                        "message": f"Invalid model size {model_size}. Available choices: {self.model_sizes}"
                    }
                )
            )
            return None
        return model_size

    def set_language(self, info):
        if info.language_probability > 0.5:
            self.language = info.language
            logging.info(f"Detected language {self.language} with probability {info.language_probability}")
            self.websocket.send(json.dumps(
                {"uid": self.client_uid, "language": self.language, "language_prob": info.language_probability}))

    def transcribe_audio(self, input_sample):
        # transcriber = self.modelsManager.get_model()
        # logging.info("transcribe_audio : " + str(time.time()))
        # result, info = transcriber['value'].transcribe(
        #     input_sample,
        #     initial_prompt=self.initial_prompt,
        #     language=self.language,
        #     task=self.task,
        #     vad_filter=self.use_vad,
        #     vad_parameters=self.vad_parameters if self.use_vad else None)
        # logging.info("transcribe_audio result : " + str(time.time()))
        #
        # self.modelsManager.add_model(transcriber['key'])

        input_data = dict()
        t = time.time()
        input_data['name'] = str(int(round(t * 1000)))
        if self.language:
            input_data['language'] = self.language

        try:
            print(self.http_url + ' : ')
            res = requests.post(self.http_url, headers=input_data, data=input_sample.tobytes())
            print(res)
            results = json.loads(res.text)
            # print(results['status'])
            if 'status' in results and results['status'] == 0:
                result = results.get('result')
                info = results.get('info')
                if self.language is None and info is not None:
                    Tanscr = namedtuple('TranscriptionInfo', info.keys())
                    self.set_language(Tanscr(**info))
                return result
            else:
                print(results)
                return None
        except Exception as e:
            print(e)
            return None

    def transcribe_audio2(self, input_sample):
        if ServeClientFasterWhisper.SINGLE_MODEL:
            ServeClientFasterWhisper.SINGLE_MODEL_LOCK.acquire()
        logging.info("transcribe_audio : " + str(time.time()))
        result, info = self.transcriber.transcribe(
            input_sample,
            initial_prompt=self.initial_prompt,
            language=self.language,
            task=self.task,
            vad_filter=self.use_vad,
            vad_parameters=self.vad_parameters if self.use_vad else None)
        logging.info("transcribe_audio result : " + str(time.time()))
        if ServeClientFasterWhisper.SINGLE_MODEL:
            ServeClientFasterWhisper.SINGLE_MODEL_LOCK.release()

        if self.language is None and info is not None:
            self.set_language(info)
        return result

    def get_previous_output(self):
        segments = []
        if self.t_start is None:
            self.t_start = time.time()
        if time.time() - self.t_start < self.show_prev_out_thresh:
            segments = self.prepare_segments()

        if len(self.text) and self.text[-1] != '':
            if time.time() - self.t_start > self.add_pause_thresh:
                self.text.append('')
        return segments

    def handle_transcription_output(self, result, duration, is_end=False):
        segments = []
        # new_result = []
        if len(result):
            self.t_start = None
            last_segment = self.update_segments(result, duration)
            segments = self.prepare_segments(last_segment)
        else:
            segments = self.get_previous_output()

        if len(segments):
            # segments.append(new_result)
            self.send_transcription_to_client(segments, is_end)

    def speech_to_text(self):
        is_end = False
        result_none = 0
        while True:
            if self.exit:
                logging.info("Exiting speech to text thread : " + str(time.time()))
                break

            if self.frames_np is None:
                continue

            self.clip_audio_if_no_valid_segment()
            input_bytes, duration, all_length = self.get_audio_chunk_for_processing()
            # tmp = len(input_bytes)
            print('--------------------------' + str(self.timestamp_offset) + '; ' + str(self.frames_offset))
            print(str(duration) + ' --- ' + str(all_length))
            if all_length == self.frames_np_size and self.frames_np_size > 0:
                is_end = True
            if duration < 0.5:
                time.sleep(0.01)
                continue
            try:
                t1 = time.time()
                input_sample = input_bytes.copy()
                result = self.transcribe_audio(input_sample)
                print('transcribe_audio : ' + str(time.time() - t1))
                if result is None or self.language is None:
                    if result_none > 2:
                        self.timestamp_offset += 0.5
                        result_none = 0
                    else:
                        result_none += 1
                    time.sleep(0.2)
                    continue
                result_none = 0
                new_result = []
                for item in result:
                    Segment_info = namedtuple('Segment', item.keys())
                    new_result.append(Segment_info(**item))

                print(new_result)
                self.handle_transcription_output(new_result, duration, is_end)

                if is_end:
                    self.canExit = True
                    time.sleep(0.1)
                    self.exit = True
                else:
                    self.canExit = False
            except Exception as e:
                logging.error(f"[ERROR]: Failed to transcribe audio chunk: {e}")
                time.sleep(0.01)

    def format_segment(self, start, end, text):
        return {
            'start': "{:.3f}".format(start),
            'end': "{:.3f}".format(end),
            'text': text
        }

    def update_segments(self, segments, duration):
        offset = None
        self.current_out = ''
        if len(segments) > 1:
            for i, s in enumerate(segments[:-1]):
                text_ = s.text
                self.text.append(text_)
                start, end = self.timestamp_offset + s.start, self.timestamp_offset + min(duration, s.end)

                if start >= end:
                    continue
                if s.no_speech_prob > self.no_speech_thresh:
                    continue
                temp_result = self.format_segment(start, end, text_)
                self.transcript.append(temp_result)
                # new_result.append(temp_result)

                offset = min(duration, s.end)

        self.current_out += segments[-1].text
        last_segment = self.format_segment(
            self.timestamp_offset + segments[-1].start,
            self.timestamp_offset + min(duration, segments[-1].end),
            self.current_out
        )
        # new_result.append(last_segment)
        if self.current_out.strip() == self.prev_out.strip() and self.current_out != '':
            self.same_output_threshold += 1
        else:
            self.same_output_threshold = 0

        if self.same_output_threshold > 5:
            if not len(self.text) or self.text[-1].strip().lower() != self.current_out.strip().lower():
                self.text.append(self.current_out)
                self.transcript.append(self.format_segment(
                    self.timestamp_offset,
                    self.timestamp_offset + duration,
                    self.current_out
                ))
            self.current_out = ''
            offset = duration
            self.same_output_threshold = 0
            last_segment = None
        else:
            self.prev_out = self.current_out

        if offset is not None:
            self.timestamp_offset += offset

        return last_segment
